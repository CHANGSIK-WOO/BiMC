SEED: 1

DEVICE:
  DEVICE_NAME: "cuda"
  GPU_ID: "0;"

DATALOADER:
  TRAIN:
    BATCH_SIZE_BASE: 64  # Normal batch size for evaluation
    BATCH_SIZE_INC: 64   # Normal batch size for evaluation
  TEST:
    BATCH_SIZE: 64      # Normal batch size for evaluation
  NUM_WORKERS: 4


MODEL:
  BACKBONE:
    NAME: "ViT-B/16"  #  RN50  RN101  ViT-B/32  ViT-B/16  ViT-L/14


TRAINER:
  BiMC:
    METHOD: "bimc_prompt"  # BiMC with prompt-based meta-learning (no edge features)
    PREC: "fp32"  # fp16, fp32, amp

    VISION_CALIBRATION: True
    LAMBDA_I: 0.1
    TAU: 16

    TEXT_CALIBRATION: True
    LAMBDA_T: 0.5

    GAMMA_BASE: 1.0
    GAMMA_INC: 5.0

    # Meta-learning specific parameters (prompt-based)
    META:
      ENABLED: True
      NUM_EPISODES: 100  # Number of meta-training episodes (base task only)
      BASE_FINETUNE_EPOCHS: 5   # Fine-tuning epochs for base task (after meta-learning)
      INC_EPOCHS: 10      # Number of training epochs for incremental tasks (standard training)
      INNER_LR: 0.01     # Learning rate for inner loop (prompt adaptation on base task)
      OUTER_LR: 0.001    # Learning rate for outer loop (meta-update on base task)
      INNER_STEPS: 10    # Number of inner loop gradient steps

      # Base session split (task 1): Sample 35 classes randomly per episode, 5-shot
      BASE_SUPPORT_CLASSES: 28  # Support classes for base meta-learning
      BASE_QUERY_CLASSES: 7     # Query classes for base meta-learning
      SUPPORT_SHOT: 5    # Number of samples per class (5-shot setting)

      # Prompt parameters
      PROMPT_LENGTH: 4        # Number of learnable prompt tokens
      PROMPT_DIM: 768         # Dimension of each prompt token (ViT-B/16 hidden dim)
      BATCH_SIZE: 4          # Batch size for meta-learning (base task)
      BASE_BATCH_SIZE: 64    # Batch size for base task fine-tuning (full-shot)
      INC_BATCH_SIZE: 64     # Batch size for incremental tasks (standard training)

    # No ensemble for prompt-based approach
    USING_ENSEMBLE: False
